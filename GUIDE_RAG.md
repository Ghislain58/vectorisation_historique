ğŸ“š GUIDE RAG â€” Vectorisation Historique (Version ComplÃ¨te)

Moteur RAG mÃ©diÃ©val â€” FAISS + Embeddings E5-Large + LLM local (Ollama)

ğŸš€ 1. Lancer une requÃªte RAG

Câ€™est la commande principale :

source venv/bin/activate
python scripts/rag_query_pipeline.py -q "Votre question ici" --top-k 5


Exemple :

python scripts/rag_query_pipeline.py \
  -q "Quelle place occupe Brioude dans les rÃ©seaux aristocratiques au IXáµ‰ siÃ¨cle ?" \
  --top-k 8


Cette commande fait :

Embedding de la question (E5-large)

Recherche vectorielle FAISS

SÃ©lection des top-k chunks pertinents

Construction dâ€™un contexte historique sourcÃ©

Appel au LLM (Ollama, modÃ¨le : llama3:latest)

Retourne :

une rÃ©ponse synthÃ©tique

les sources, pages, chunks et scores FAISS

un discours historien strict (pas dâ€™hallucinations)

ğŸ›ï¸ 2. RÃ©glages importants et subtilitÃ©s de recherche
ğŸ”¥ --top-k (pertinence et largeur de contexte)

5 â†’ prÃ©cis, idÃ©al pour question ciblÃ©e

8â€“12 â†’ questions de synthÃ¨se, thÃ¨mes larges

20+ â†’ exploration, comparaison, panoramas

ğŸ’¡ Recommandation historique :
top-k = 8 â†’ Ã©quilibre idÃ©al (peu de bruit, assez de contexte)

ğŸ”¥ TempÃ©rature LLM

RÃ©glÃ©e dans llm_client.py :

0.0 â†’ trÃ¨s strict, non-crÃ©atif

0.1â€“0.2 â†’ idÃ©al pour rÃ©ponse historique fiable

0.4+ â†’ plus interprÃ©tatif (Ã©viter)

ğŸ‘‰ Pour lâ€™histoire mÃ©diÃ©vale : 0.1, câ€™est parfait.

ğŸ”¥ ModÃ¨le LLM (Ollama)

Par dÃ©faut : llama3:latest

Changer :

ollama pull llama3


Et dans llm_client.py :

model="llama3:latest"


Autres possibilitÃ©s :

mistral:latest

llama3.1 (si tu veux tester)

modÃ¨les API (OpenAI, Groq) â†’ en modifiant LLMConfig

ğŸ”¥ SubtilitÃ©s de recherche historique
âœ” Formuler une question prÃ©cise

FAISS adore :

lieux > personnes > dates

syntagmes nominaux longs

Ã©vÃªques, abbayes, cartulaires, actes prÃ©cis

Exemples :

âŒ â€œLes monastÃ¨res en Auvergne ?â€
âœ” â€œQuels acteurs ecclÃ©siastiques apparaissent dans lâ€™implantation des monastÃ¨res dâ€™Auvergne au IXáµ‰ siÃ¨cle ?â€

âœ” Ajouter un acteur clÃ©

Exemple :

â€œAvitâ€

â€œGÃ©raud dâ€™Aurillacâ€

â€œSidoine Apollinaireâ€

â€œÃ©vÃªques de Clermontâ€

â¡ï¸ Excellence du matching FAISS sur anthroponymes uniques.

âœ” Ajouter une pÃ©riode ou un siÃ¨cle

Exemple :

â€œIXáµ‰ siÃ¨cleâ€

â€œpÃ©riode carolingienneâ€

â¡ï¸ Active les annÃ©es et amÃ©liore le score.

âœ” TrÃ¨s important : le RAG est strict

Ton pipeline empÃªche :

les hallucinations

les inventions de sources

les synthÃ¨ses hors contexte

Le LLM ne peut rÃ©pondre que sur ce que FAISS lui donne.

ğŸ§± 3. Reconstruction du pipeline (si tu ajoutes des PDF)

Quand tu ajoutes de nouveaux PDF dans :

data/sources/local/pdf/
data/sources/bnf_gallica/pdf/


Il faut reconstruire le corpus + FAISS.

1ï¸âƒ£ GÃ©nÃ©rer les chunks
python scripts/build_corpus_jsonl.py


Produit :

data/chunks/standard/corpus_chunks.jsonl


Inclut :

texte

pages

doc_id

mÃ©tadonnÃ©es

entitÃ©s dÃ©tectÃ©es

annÃ©es min/max

2ï¸âƒ£ Construire lâ€™index FAISS
python scripts/build_faiss_index.py


Produit :

data/embeddings/e5_large/index.faiss
data/embeddings/e5_large/index_ids.json

ğŸ” 4. Tester FAISS sans LLM

TrÃ¨s utile pour vÃ©rifier les rÃ©sultats :

python scripts/debug_search.py


Affiche :

les meilleurs chunks

les scores de similaritÃ©

les mÃ©tadonnÃ©es

les entitÃ©s dÃ©tectÃ©es

Permet de valider :

cohÃ©rence du corpus

qualitÃ© des embeddings

efficacitÃ© du lexique

ğŸ¤– 5. Tester le modÃ¨le Ollama

Pour vÃ©rifier la connectivitÃ© :

python scripts/test_ollama_llm.py


Ou via curl :

curl http://localhost:11434/api/generate -d '{
  "model": "llama3:latest",
  "prompt": "Test de connexion."
}'

ğŸ§© 6. Fonctionnement interne du pipeline
ğŸ“¥ 1. Ingestion

Module :
src/medieval_rag/ingestion/loaders.py

Extraction page par page

Nettoyage lÃ©ger

OCR non obligatoire (mais possible plus tard)

ğŸ§© 2. Chunking (v1 standard)

Module : ingestion/chunker.py

~1800 caractÃ¨res

overlap ~200

adaptÃ© Ã  E5-large

stable et robuste

ğŸ§­ 3. DÃ©tection dâ€™entitÃ©s

Module : enrichment/entities.py

DÃ©tecte :

personnes (lexique JSON local)

lieux (lexique local)

annÃ©es (regex intelligente)

nettoie : chiffres, ponctuation, tokens faibles

enrichit chaque chunk

Lexique attendu :

mes_documents_historiques/entity_lexicon_v2.json


Structure :

{
  "persons": ["Avit", "GÃ©raud d'Aurillac"],
  "places": ["Brioude", "Clermont"],
  "years": ["700", "800", "900"]
}

ğŸ§  4. Embeddings E5-large

Modules :

embeddings/model_loader.py

embeddings/embedder.py

CaractÃ©ristiques :

modÃ¨le SOTA pour texte long

optimisÃ© pour similaritÃ© sÃ©mantique

GPU si disponible

batch automatique selon ta VRAM

ğŸ“¡ 5. Index FAISS

Module : build_faiss_index.py

Index :

mÃ©trique : dot product / inner product

normalisation des embeddings

IDs synchronisÃ©s avec JSONL

ğŸ¤– 6. Pipeline RAG final

Module : rag/rag_pipeline.py

SÃ©quence :

Embedding de la requÃªte

Recherche FAISS

Assemblage du contexte (pages ordonnÃ©es)

Prompt â€œhistorien strictâ€

Envoi Ã  Ollama

RÃ©ponse sourcÃ©e + chunks + scores

ğŸ§ª 7. Suite dâ€™Ã©valuation RAG (optionnelle)

Script :

python scripts/rag_eval_suite.py


Permet :

tests automatiques

comparer pipelines

mesurer rÃ©gressions

valider qualitÃ© des rÃ©ponses

ğŸ›  8. RÃ©solution des problÃ¨mes
âŒ index.faiss not found

â†’ reconstruire :

python scripts/build_faiss_index.py

âŒ corpus_chunks.jsonl not found

â†’ reconstruire :

python scripts/build_corpus_jsonl.py

âŒ Ollama refuse la requÃªte

â†’ vÃ©rifier :

ollama list
ollama pull llama3

âŒ CUDA OOM

â†’ diminuer batch_size dans embedder.py
â†’ fermer Chrome / TouchDesigner / Resolve

ğŸ›ï¸ 9. Bonnes pratiques de recherche mÃ©diÃ©vale
âœ” Citer systÃ©matiquement un lieu ou un acteur

FAISS adore Ã§a.

âœ” Mentionner le IXáµ‰ ou Xáµ‰ siÃ¨cle

Les annÃ©es min/max amÃ©liorent le score.

âœ” Ã‰viter les questions vagues

FAISS ne peut rien matcher avec â€œparle-moi dâ€™Aurillacâ€.

âœ” PrÃ©fÃ©rer les formulations :

â€œQuels acteursâ€

â€œQuelle place occupeâ€

â€œQuels Ã©lÃ©ments montrent queâ€

âœ” Pour la macro-histoire â†’ top-k Ã©levÃ©

migrations

tendances sociales

rÃ©seaux aristocratiques

âœ” Pour la micro-histoire â†’ top-k bas

un acte

un Ã©vÃªque

un monastÃ¨re prÃ©cis

ğŸ Fin du GUIDE RAG