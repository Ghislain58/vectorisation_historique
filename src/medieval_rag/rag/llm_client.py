# src/medieval_rag/rag/llm_client.py
from __future__ import annotations

import json
from dataclasses import dataclass
from typing import Optional, Dict, Any

import requests


@dataclass
class LLMConfig:
    """
    Configuration minimale pour le client Ollama LLM.
    Tu peux changer le mod√®le ici pour que tout le projet utilise le m√™me.
    """
    model: str = "llama3:latest"
    base_url: str = "http://localhost:11434"
    temperature: float = 0.2
    max_tokens: int = 512
    timeout: int = 120  # secondes


class OllamaLLMClient:
    """
    Client l√©ger pour envoyer des requ√™tes √† Ollama via /api/chat.
    Compatible avec les prompts syst√®me + utilisateur du pipeline RAG.
    """

    def __init__(self, config: Optional[LLMConfig] = None) -> None:
        self.config = config or LLMConfig()
        print(f"üîç LLM utilis√© par le pipeline : {self.config.model}")

    # ------------------------------------------------------------------
    # M√©thode principale : g√©n√©ration d‚Äôun texte via Ollama
    # ------------------------------------------------------------------
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        extra_params: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Appelle Ollama via /api/chat.
        Retourne uniquement le contenu textuel g√©n√©r√©.
        """

        url = f"{self.config.base_url.rstrip('/')}/api/chat"

        payload: Dict[str, Any] = {
            "model": self.config.model,
            "temperature": temperature if temperature is not None else self.config.temperature,
            "num_predict": max_tokens if max_tokens is not None else self.config.max_tokens,
            "stream": False,
            "messages": [],
        }

        # Prompt syst√®me (optionnel mais essentiel pour le mode "historien strict")
        if system_prompt:
            payload["messages"].append({"role": "system", "content": system_prompt})

        # Prompt utilisateur
        payload["messages"].append({"role": "user", "content": prompt})

        # Permet de passer d‚Äôautres param√®tres si besoin
        if extra_params:
            payload.update(extra_params)

        # ------------------ Requ√™te vers Ollama ------------------
        try:
            response = requests.post(url, json=payload, timeout=self.config.timeout)
        except requests.RequestException as e:
            raise RuntimeError(
                f"Erreur de connexion √† Ollama ({url}) : {e}"
            ) from e

        if response.status_code != 200:
            raise RuntimeError(
                f"Erreur HTTP {response.status_code} depuis Ollama : {response.text}"
            )

        # ------------------ Analyse de la r√©ponse ------------------
        try:
            data = response.json()
        except json.JSONDecodeError as e:
            raise RuntimeError(f"R√©ponse non JSON depuis Ollama : {response.text}") from e

        # Format standard pour /api/chat
        message = data.get("message") or {}
        content = message.get("content")

        if not content:
            raise RuntimeError(f"Aucun contenu g√©n√©r√© par Ollama : {data}")

        return content
