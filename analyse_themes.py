#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
analyse_themes.py
-----------------

Analyse thÃ©matique massive sur le corpus indexÃ© dans Chroma.

- On fournit une liste de termes (latin, franÃ§ais, etc.).
- Le script parcourt tous les chunks de la collection "documents_historiques".
- Il compte les occurrences de chaque terme :
    - total global
    - par type de document (cartulaire / charte / acadÃ©mique / autres)
    - par "dossier logique" (articles, cartulaires, theses, ...)

- Il affiche aussi quelques exemples de chunks oÃ¹ chaque terme apparaÃ®t.

Usage :

    python analyse_themes.py
        -> utilise une liste de termes par dÃ©faut (villa, mansus, ecclesia...)

    python analyse_themes.py --terms "villa,mansus,feodum,ecclesia"
        -> analyse ces termes-lÃ 

    python analyse_themes.py --terms "decima,terra castrum"
        -> gÃ¨re aussi des expressions avec espaces (simple recherche textuelle)

"""

import argparse
import re
from collections import Counter, defaultdict
from pathlib import PurePosixPath
from typing import Dict, List, Tuple

from chromadb import PersistentClient


def guess_folder_from_source(source: str) -> str:
    """
    Essaie de dÃ©duire le â€œdossier logiqueâ€ Ã  partir du chemin complet.

    /home/.../mes_documents_historiques/cartulaires/MonPDF.pdf
    -> "cartulaires"
    """
    try:
        p = PurePosixPath(source)
    except Exception:
        return "UNKNOWN"

    parts = list(p.parts)
    if "mes_documents_historiques" in parts:
        idx = parts.index("mes_documents_historiques")
        if idx + 1 < len(parts):
            return parts[idx + 1]
    return "UNKNOWN"


def iter_batches(collection, batch_size: int = 500):
    """
    ItÃ¨re sur la collection par paquets (documents + metadatas),
    pour Ã©viter de tout charger en mÃ©moire d'un coup.
    """
    offset = 0
    while True:
        res = collection.get(
            include=["documents", "metadatas"],
            limit=batch_size,
            offset=offset,
        )
        docs = res.get("documents", [])
        metas = res.get("metadatas", [])

        if not docs:
            break

        # docs et metas sont des listes parallÃ¨les
        for doc, meta in zip(docs, metas):
            yield doc, meta

        offset += len(docs)


def compile_patterns(terms: List[str]) -> Dict[str, re.Pattern]:
    """
    PrÃ©pare des regex insensibles Ã  la casse pour chaque terme.
    On utilise une recherche simple sur la chaÃ®ne, sans gestion
    fine de lemmatisation, parce qu'on travaille sur du brut.
    """
    patterns = {}
    for term in terms:
        t = term.strip()
        if not t:
            continue
        # On Ã©chappe le terme pour Ã©viter les surprises regex
        pat = re.compile(re.escape(t), re.IGNORECASE)
        patterns[term] = pat
    return patterns


def highlight_snippet(text: str, term: str, width: int = 200) -> str:
    """
    Renvoie un petit extrait du chunk oÃ¹ le terme apparaÃ®t.
    On coupe autour de la premiÃ¨re occurrence.
    """
    if not text:
        return ""
    lower = text.lower()
    idx = lower.find(term.lower())
    if idx == -1:
        # pas trouvÃ©, on coupe juste le dÃ©but
        return (text[:width] + "â€¦") if len(text) > width else text
    start = max(0, idx - width // 2)
    end = min(len(text), idx + width // 2)
    snippet = text[start:end]
    return snippet.replace("\n", " ").strip() + ("â€¦" if end < len(text) else "")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--terms",
        type=str,
        default="villa,mansus,feodum,ecclesia,decima,castrum,pagus",
        help="Liste de termes sÃ©parÃ©s par des virgules (latin ou autre)",
    )
    args = parser.parse_args()

    terms = [t.strip() for t in args.terms.split(",") if t.strip()]
    if not terms:
        print("âš ï¸  Aucun terme valide fourni.")
        return

    print("ðŸ”— Connexion Ã  la base Chromaâ€¦")
    client = PersistentClient(path="./vector_db_historique")
    col = client.get_or_create_collection("documents_historiques")

    print(f"ðŸ” Analyse des thÃ¨mes : {', '.join(terms)}")
    patterns = compile_patterns(terms)

    # Compteurs globaux
    total_chunks = 0
    counts_global = Counter()
    counts_by_type: Dict[str, Counter] = defaultdict(Counter)
    counts_by_folder: Dict[str, Counter] = defaultdict(Counter)

    # On garde quelques exemples (top chunks) pour chaque terme
    examples: Dict[str, List[Tuple[int, str, str]]] = defaultdict(list)
    # format : (nombre_occurrences_dans_chunk, fichier, extrait)

    print("ðŸ“¥ Parcours de tous les chunksâ€¦")
    for doc, meta in iter_batches(col, batch_size=500):
        total_chunks += 1
        if not doc:
            continue

        text = doc
        source = meta.get("source", "") or ""
        filename = meta.get("filename", "") or source or "inconnu"

        folder = guess_folder_from_source(source)

        if meta.get("is_cartulary"):
            doc_type = "cartulary"
        elif meta.get("is_charter"):
            doc_type = "charter"
        elif meta.get("is_academic"):
            doc_type = "academic"
        else:
            doc_type = "other"

        # Comptage pour chaque terme
        for term, pat in patterns.items():
            matches = pat.findall(text)
            n = len(matches)
            if n <= 0:
                continue

            # Mise Ã  jour des compteurs
            counts_global[term] += n
            counts_by_type[term][doc_type] += n
            counts_by_folder[term][folder] += n

            # On stocke quelques exemples (max ~5 par terme)
            if len(examples[term]) < 5:
                snippet = highlight_snippet(text, term)
                examples[term].append((n, filename, snippet))

    print(f"\nâœ… Analyse terminÃ©e sur {total_chunks} chunks.\n")

    # === RÃ©sumÃ© par terme ===
    for term in terms:
        print("=" * 70)
        print(f"ðŸ§¾ Terme : {term}")
        total = counts_global.get(term, 0)
        print(f"  âžœ Occurrences totales : {total}")

        # Par type de document
        ctype = counts_by_type.get(term, {})
        if ctype:
            print("  ðŸ“‚ Par type de document :")
            for t, c in ctype.most_common():
                print(f"    - {t:10s} : {c}")
        else:
            print("  ðŸ“‚ Par type de document : (aucune occurrence)")

        # Par dossier logique
        cfold = counts_by_folder.get(term, {})
        if cfold:
            print("  ðŸ—‚ï¸  Par dossier (mes_documents_historiques/â€¦):")
            for f, c in cfold.most_common():
                print(f"    - {f:15s} : {c}")
        else:
            print("  ðŸ—‚ï¸  Par dossier : (aucune occurrence)")

        # Exemples
        ex_list = examples.get(term, [])
        if ex_list:
            print("  ðŸ” Exemples de chunks oÃ¹ le terme apparaÃ®t :")
            for n, fname, snippet in ex_list:
                print(f"    â€¢ [{n}Ã—] {fname}")
                print(f"      Â« {snippet} Â»")
        else:
            print("  ðŸ” Aucun exemple stockÃ© (aucune occurrence trouvÃ©e).")

        print()

    print("ðŸ Fin de l'analyse thÃ©matique.")


if __name__ == "__main__":
    main()
