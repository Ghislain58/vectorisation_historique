ğŸ“˜ Vectorisation Historique & Moteur RAG MÃ©diÃ©val

Pipeline complet pour construire un assistant mÃ©diÃ©val basÃ© sur un corpus scientifique (thÃ¨ses, articles, PDF Gallica).
Le systÃ¨me rÃ©alise :

extraction de texte (PDF locaux + Gallica)

chunking structurÃ©

embeddings (E5-large)

index vectoriel FAISS

recherche sÃ©mantique

RAG (LLM API ou local via Ollama)

OptimisÃ© pour WSL + Python + GPU CUDA.

ğŸ“ Architecture du projet
vectorisation_historique/
â”‚
â”œâ”€â”€ src/medieval_rag/
â”‚   â”œâ”€â”€ ingestion/        # loaders PDF + chunker
â”‚   â”œâ”€â”€ enrichment/       # extraction dâ€™entitÃ©s
â”‚   â”œâ”€â”€ embeddings/       # modÃ¨le E5 + embedder
â”‚   â””â”€â”€ rag/              # LLM client + pipeline RAG
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_corpus_jsonl.py
â”‚   â”œâ”€â”€ build_faiss_index.py
â”‚   â”œâ”€â”€ debug_search.py
â”‚   â”œâ”€â”€ inspect_entities.py
â”‚   â””â”€â”€ rag_query_llm.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sources/          # PDF d'entrÃ©e (non versionnÃ©s)
â”‚   â”œâ”€â”€ chunks/           # corpus JSONL gÃ©nÃ©rÃ©
â”‚   â””â”€â”€ embeddings/       # index FAISS gÃ©nÃ©rÃ©
â”‚
â”œâ”€â”€ requirements.in
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements_freeze.txt
â”œâ”€â”€ DEPENDENCIES.md
â””â”€â”€ README.md

âš™ï¸ Installation
1. Cloner le projet
git clone git@github.com:Ghislain58/vectorisation_historique.git
cd vectorisation_historique

2. Installer lâ€™environnement Python
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt


(Vous pouvez utiliser requirements_freeze.txt pour une installation reproductible.)

â™»ï¸ Reconstruction des donnÃ©es (aprÃ¨s un clone)

Le dÃ©pÃ´t ne contient pas :

PDF

corpus_chunks.jsonl

index FAISS

Ces fichiers doivent Ãªtre gÃ©nÃ©rÃ©s Ã  nouveau localement.

1. Placer les PDF

DÃ©poser vos PDF dans :

data/sources/local/pdf/       # ThÃ¨ses, articles, sources personnelles
data/sources/bnf_gallica/pdf/ # PDF Gallica (optionnel)

2. GÃ©nÃ©rer le corpus JSONL (chunks + embeddings)
source venv/bin/activate
python scripts/build_corpus_jsonl.py


Sortie :

data/chunks/standard/corpus_chunks.jsonl

3. Construire lâ€™index FAISS
python scripts/build_faiss_index.py


Sorties :

data/embeddings/e5_large/index.faiss
data/embeddings/e5_large/index_ids.json

4. Tester la recherche sÃ©mantique
python scripts/debug_search.py


Permet de vÃ©rifier :

les rÃ©sultats de recherche FAISS

la cohÃ©rence des textes

la dÃ©tection des entitÃ©s

5. Faire une requÃªte RAG (LLM API ou local)
python scripts/rag_query_llm.py


Pipeline :

Embedding de la question

Recherche FAISS

Construction du contexte

Appel Ã  un LLM (OpenAI, Groq, Ollamaâ€¦)

RÃ©ponse historique sourcÃ©e et sans hallucinations

ğŸ”Œ IntÃ©gration LLM local (Ollama)

Pour utiliser un modÃ¨le local :

ollama pull llama3.1


Puis configurer :

src/medieval_rag/rag/llm_client.py

mode = "ollama"
model = "llama3.1"

ğŸ” Outils de vÃ©rification des entitÃ©s
python scripts/inspect_entities.py


Permet dâ€™amÃ©liorer :

lexique des personnes

lieux mÃ©diÃ©vaux

annÃ©es

Ã©limination des faux positifs (â€œditâ€, â€œcomteâ€, etc.)

ğŸ§ª Tests rapides
GÃ©nÃ©rer un mini-corpus de test
python scripts/build_corpus_jsonl.py --debug

Interroger sans LLM
python scripts/debug_search.py

ğŸ›  Workflow Git
Ajouter des fichiers
git add .
git commit -m "feat: nouvelle fonctionnalitÃ©"
git push


Branches recommandÃ©es :

main  â†’ stable
dev   â†’ dÃ©veloppement
feat/... â†’ nouvelles fonctionnalitÃ©s

ğŸ¯ Objectif du projet

CrÃ©er un assistant mÃ©diÃ©val autonome, capable de :

rÃ©pondre Ã  des questions historiques complexes

analyser des sources primaires et secondaires

citer pages, documents, titres

fonctionner totalement hors-ligne

enrichir ton projet artistique / numÃ©rique Symbioware / CogniLink

âœ”ï¸ Fin du README