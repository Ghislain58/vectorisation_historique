Vectorisation Historique & Moteur RAG MÃ©diÃ©val

SystÃ¨me complet pour analyser un corpus scientifique mÃ©diÃ©val (thÃ¨ses, articles, actes, PDF Gallica) et fournir des rÃ©ponses sourcÃ©es via un moteur RAG (Retrieval Augmented Generation).

OptimisÃ© pour :

WSL / Linux

Python 3.10+

CUDA

Embeddings E5-large

Index vectoriel FAISS

LLM local via Ollama

ğŸ›ï¸ Objectif du projet

Construire un assistant mÃ©diÃ©val autonome, capable de :

extraire et analyser des sources mÃ©diÃ©vales complexes,

effectuer une recherche sÃ©mantique rigoureuse dans un corpus scientifique,

citer prÃ©cisÃ©ment les pages et documents,

rÃ©duire drastiquement les hallucinations grÃ¢ce Ã  FAISS + E5 + prompt historien strict,

fonctionner totalement hors-ligne via Ollama.

Le moteur peut sâ€™intÃ©grer dans ton travail artistique / numÃ©rique (Symbioware / CogniLink / installations interactives).

ğŸ— Architecture du projet
vectorisation_historique/
â”‚
â”œâ”€â”€ src/medieval_rag/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚    â”œâ”€â”€ loaders.py        # Extraction PDF page par page
â”‚   â”‚    â””â”€â”€ chunker.py        # Chunking structurÃ© (1800 + overlap 200)
â”‚   â”‚
â”‚   â”œâ”€â”€ enrichment/
â”‚   â”‚    â””â”€â”€ entities.py       # DÃ©tection entitÃ©s mÃ©diÃ©vales : personnes, lieux, annÃ©es
â”‚   â”‚
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚    â”œâ”€â”€ model_loader.py   # Chargement E5-large (GPU/CPU)
â”‚   â”‚    â””â”€â”€ embedder.py       # Embeddings batchÃ©s (sÃ©curitÃ© VRAM)
â”‚   â”‚
â”‚   â””â”€â”€ rag/
â”‚        â”œâ”€â”€ rag_pipeline.py   # Pipeline RAG complet (FAISS + LLM)
â”‚        â””â”€â”€ llm_client.py     # Client Ollama (API chat locale)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_corpus_jsonl.py  # Pipeline ingestion â†’ chunks â†’ embeddings â†’ JSONL
â”‚   â”œâ”€â”€ build_faiss_index.py   # Construction index FAISS + index_ids.json
â”‚   â”œâ”€â”€ rag_query_pipeline.py  # Interface principale RAG (FAISS + LLM)
â”‚   â”œâ”€â”€ rag_query_llm.py       # Variante LLM-only (ancien test)
â”‚   â”œâ”€â”€ debug_search.py        # Analyse FAISS avancÃ©e (expert)
â”‚   â”œâ”€â”€ inspect_entities.py    # VÃ©rification lexique mÃ©diÃ©val
â”‚   â””â”€â”€ test_ollama_llm.py     # Test direct du modÃ¨le Ollama
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_loader.py         # Test extraction PDF
â”‚   â”œâ”€â”€ test_chunker.py        # Test chunking
â”‚   â””â”€â”€ test_embeddings.py     # Test embeddings E5
â”‚
â”œâ”€â”€ legacy/                    # ANCIEN PIPELINE (non utilisÃ©)
â”‚   â”œâ”€â”€ historical_vectorizer.py     # ObsolÃ¨te (ChromaDB)
â”‚   â”œâ”€â”€ vectorizer.py
â”‚   â”œâ”€â”€ analyse_*.py
â”‚   â””â”€â”€ autres anciens scripts
â”‚
â”œâ”€â”€ data/                      # Non versionnÃ© (voir .gitignore)
â”‚   â”œâ”€â”€ sources/               # PDF bruts
â”‚   â”œâ”€â”€ chunks/                # corpus_chunks.jsonl (gÃ©nÃ©rÃ©)
â”‚   â””â”€â”€ embeddings/            # index.faiss + index_ids.json
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements_freeze.txt
â”œâ”€â”€ DEPENDENCIES.md
â””â”€â”€ README.md

âš™ï¸ Installation
git clone git@github.com:Ghislain58/vectorisation_historique.git
cd vectorisation_historique

python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

ğŸ“‚ Sources Ã  prÃ©parer (non versionnÃ©es)

DÃ©poser vos PDF dans :

data/sources/local/pdf/
data/sources/bnf_gallica/pdf/

â™»ï¸ Reconstruction du corpus (FAISS + JSONL)

Le dÃ©pÃ´t ne contient pas les donnÃ©es lourdes suivantes :

PDF

corpus_chunks.jsonl

index.faiss / index_ids.json

Ces fichiers sont gÃ©nÃ©rÃ©s localement.

1ï¸âƒ£ GÃ©nÃ©rer le corpus JSONL (chunks + entitÃ©s + embeddings)
source venv/bin/activate
python scripts/build_corpus_jsonl.py


Sortie :

data/chunks/standard/corpus_chunks.jsonl


Ce fichier contient :

texte chunkÃ©

pages

id de document

entitÃ©s dÃ©tectÃ©es (people, places, years)

embeddings E5-large

2ï¸âƒ£ Construire lâ€™index FAISS
python scripts/build_faiss_index.py


Sorties :

data/embeddings/e5_large/index.faiss
data/embeddings/e5_large/index_ids.json

ğŸ” VÃ©rifications et outils de contrÃ´le
ğŸ” Recherche FAISS sans LLM (debug expert)
python scripts/debug_search.py


Permet dâ€™inspecter :

les distances FAISS

les chunks les plus proches

les pages correspondantes

les entitÃ©s mÃ©diÃ©vales dÃ©tectÃ©es

ğŸ” VÃ©rification du lexique mÃ©diÃ©val
python scripts/inspect_entities.py


Utile pour affiner :

noms propres mÃ©diÃ©vaux

lieux anciens

annÃ©es

Ã©liminer les faux positifs (â€œditâ€, â€œfils deâ€, etc.)

ğŸ” Tests unitaires
python scripts/test_loader.py
python scripts/test_chunker.py
python scripts/test_embeddings.py
python scripts/test_ollama_llm.py

ğŸ¤– Faire une requÃªte RAG (pipeline complet)
python scripts/rag_query_pipeline.py \
    -q "Quel rÃ´le jouent les Ã©vÃªques de Clermont dans les fondations monastiques ?" \
    --top-k 5


Pipeline :

Embedding de la requÃªte (E5-large)

Recherche vectorielle FAISS

Reconstruction des extraits

Prompt historien strict

Appel au modÃ¨le local via Ollama

RÃ©ponse sourcÃ©e et non-hallucinÃ©e

ğŸ§  Utiliser un LLM local (Ollama)

Installer un modÃ¨le :

ollama pull llama3.1


Configurer dans :

src/medieval_rag/rag/llm_client.py

ğŸ›  Workflow Git
Branches recommandÃ©es

main â†’ version stable

dev â†’ dÃ©veloppement courant

feat/... â†’ nouvelles fonctionnalitÃ©s

Cycle standard
git checkout dev
git pull
# travail...
git add .
git commit -m "feat: ..."
git push


Merge vers main via Pull Request sur GitHub.

ğŸŸ¤ Legacy (ancien pipeline)

Le dossier legacy/ contient lâ€™ancien pipeline basÃ© sur ChromaDB.
Il est conservÃ© uniquement :

pour mÃ©moire,

pour la reprise dâ€™algorithmes (OCR, heuristiques),

mais il nâ€™est pas utilisÃ© dans le pipeline FAISS actuel.

ğŸ¯ RÃ©sultat :

Un moteur RAG mÃ©diÃ©val :

propre

modulaire

stable

professionnel

entiÃ¨rement offline

adaptÃ© aux corpus massifs

extensible vers ton projet artistique ou scientifique