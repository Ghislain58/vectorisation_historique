# ğŸ“˜ Vectorisation Historique & Moteur RAG MÃ©diÃ©val  

SystÃ¨me complet pour analyser un corpus scientifique mÃ©diÃ©val (thÃ¨ses, articles, actes, PDF Gallica) et fournir des rÃ©ponses sourcÃ©es via un moteur **RAG** (Retrieval Augmented Generation).

ğŸ›ï¸ Objectif du projet
Construire un assistant mÃ©diÃ©val autonome, capable de :

interroger un corpus scientifique mÃ©diÃ©val,

effectuer une recherche sÃ©mantique rigoureuse,

citer prÃ©cisÃ©ment pages et documents,

limiter les hallucinations (FAISS + E5 + prompt historien strict),

fonctionner hors-ligne avec un LLM local via Ollama.

Le moteur peut ensuite sâ€™intÃ©grer dans des projets numÃ©riques / artistiques (Symbioware / CogniLink, installations interactives, etc.).



OptimisÃ© pour :

- WSL / Linux  
- Python 3.10+  
- CUDA  
- Embeddings E5-large  
- Index vectoriel FAISS  
- LLM local via **Ollama**

---


ğŸ— Architecture du projet

vectorisation_historique/
â”‚
â”œâ”€â”€ src/medieval_rag/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚    â”œâ”€â”€ loaders.py        # Extraction PDF page par page
â”‚   â”‚    â””â”€â”€ chunker.py        # Chunking structurÃ© (1800 + overlap 200)
â”‚   â”‚
â”‚   â”œâ”€â”€ enrichment/
â”‚   â”‚    â””â”€â”€ entities.py       # EntitÃ©s mÃ©diÃ©vales : personnes, lieux, annÃ©es
â”‚   â”‚
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚    â”œâ”€â”€ model_loader.py   # Chargement E5-large (GPU/CPU)
â”‚   â”‚    â””â”€â”€ embedder.py       # Embeddings batchÃ©s (sÃ©curitÃ© VRAM)
â”‚   â”‚
â”‚   â””â”€â”€ rag/
â”‚        â”œâ”€â”€ rag_pipeline.py   # Pipeline RAG complet (FAISS + LLM)
â”‚        â””â”€â”€ llm_client.py     # Client Ollama (API chat locale)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_corpus_jsonl.py  # Ingestion â†’ chunks â†’ embeddings â†’ JSONL
â”‚   â”œâ”€â”€ build_faiss_index.py   # Construction index.faiss + index_ids.json
â”‚   â”œâ”€â”€ rag_query_pipeline.py  # Interface principale RAG (FAISS + LLM)
â”‚   â”œâ”€â”€ rag_query_llm.py       # Variante plus simple (LLM configurable)
â”‚   â”œâ”€â”€ debug_search.py        # Analyse FAISS avancÃ©e
â”‚   â”œâ”€â”€ inspect_entities.py    # VÃ©rification lexique mÃ©diÃ©val
â”‚   â””â”€â”€ test_ollama_llm.py     # Test direct du modÃ¨le Ollama
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_loader.py         # Test extraction PDF
â”‚   â”œâ”€â”€ test_chunker.py        # Test chunking
â”‚   â””â”€â”€ test_embeddings.py     # Test embeddings E5
â”‚
â”œâ”€â”€ legacy/                    # ANCIEN PIPELINE (non utilisÃ©)
â”‚   â”œâ”€â”€ historical_vectorizer.py
â”‚   â”œâ”€â”€ vectorizer.py
â”‚   â”œâ”€â”€ analyse_*.py
â”‚   â””â”€â”€ autres anciens scripts
â”‚
â”œâ”€â”€ data/                      # Non versionnÃ© (voir .gitignore)
â”‚   â”œâ”€â”€ sources/               # PDF bruts
â”‚   â”œâ”€â”€ chunks/                # corpus_chunks.jsonl (gÃ©nÃ©rÃ©)
â”‚   â””â”€â”€ embeddings/            # index.faiss + index_ids.json
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements_freeze.txt
â”œâ”€â”€ DEPENDENCIES.md
â””â”€â”€ README.md
âš™ï¸ Installation

git clone git@github.com:Ghislain58/vectorisation_historique.git
cd vectorisation_historique

python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
ğŸ“‚ PrÃ©parer les sources (non versionnÃ©es)
DÃ©poser les PDF dans :


data/sources/local/pdf/        # ThÃ¨ses, articles, PDF perso
data/sources/bnf_gallica/pdf/  # PDF Gallica
â™»ï¸ Reconstruction du corpus (FAISS + JSONL)
1ï¸âƒ£ GÃ©nÃ©rer le corpus JSONL (chunks + entitÃ©s + embeddings)

source venv/bin/activate
python scripts/build_corpus_jsonl.py
Sortie :
data/chunks/standard/corpus_chunks.jsonl
Ce JSONL contient pour chaque chunk :

texte,

numÃ©ro(s) de page,

doc_id,

mÃ©tadonnÃ©es (source, titreâ€¦),

entitÃ©s (personnes / lieux / annÃ©es),

embedding E5-large.

2ï¸âƒ£ Construire lâ€™index FAISS

python scripts/build_faiss_index.py
Sorties :


data/embeddings/e5_large/index.faiss
data/embeddings/e5_large/index_ids.json
ğŸ” VÃ©rifications et debug
ğŸ” Test extraction PDF

python tests/test_loader.py
ğŸ” Test chunking

python tests/test_chunker.py
ğŸ” Test embeddings E5-large

python tests/test_embeddings.py
ğŸ” Test LLM local (Ollama)

python scripts/test_ollama_llm.py
Affiche le modÃ¨le utilisÃ© (ex : mistral:latest) et une rÃ©ponse de test.

ğŸ” Debug FAISS avancÃ©

python scripts/debug_search.py
Permet de :

inspecter les rÃ©sultats FAISS,

contrÃ´ler les distances,

vÃ©rifier les pages et les textes,

voir les entitÃ©s dÃ©tectÃ©es.

ğŸ¤– Faire une requÃªte RAG complÃ¨te
Commande principale

python scripts/rag_query_pipeline.py \
    -q "Quel rÃ´le jouent les Ã©vÃªques de Clermont dans l'implantation des monastÃ¨res d'Auvergne ?" \
    --top-k 5
ParamÃ¨tres importants :

-q / --query : ta question en langage naturel

--top-k : nombre de chunks les plus pertinents Ã  rÃ©cupÃ©rer

Pipeline exÃ©cutÃ© :

Embedding de la requÃªte (intfloat/multilingual-e5-large)

Recherche vectorielle FAISS

RÃ©cupÃ©ration des chunks JSONL correspondants

Construction dâ€™un prompt â€œhistorien strict, sourcÃ©â€

Appel Ã  Ollama via llm_client.py

Affichage :

RÃ‰PONSE DU LLM

SOURCES UTILISÃ‰ES (docs + pages + scores)

ğŸ¤– ModÃ¨les locaux (Ollama)
Lister les modÃ¨les :


ollama list
Installer un modÃ¨le (exemples) :


ollama pull llama3
ollama pull mistral
Le modÃ¨le utilisÃ© par dÃ©faut dans le pipeline RAG est configurÃ© dans :


src/medieval_rag/rag/llm_client.py
Tu peux adapter :

model = "llama3:latest"
ou

model = "mistral:latest"

ğŸ›  Workflow Git
Branches recommandÃ©es :

main â†’ version stable

dev â†’ dÃ©veloppement courant

feat/... â†’ nouvelles fonctionnalitÃ©s

Exemple de cycle :


git checkout dev
git pull

# travail...

git add .
git commit -m "feat: amÃ©lioration pipeline RAG"
git push origin dev
Merge vers main via Pull Request sur GitHub.

ğŸŸ¤ Legacy
Le dossier legacy/ contient lâ€™ancien pipeline basÃ© sur ChromaDB.
Il est conservÃ© pour mÃ©moire (OCR, heuristiques, explorations),

mais il nâ€™est plus utilisÃ© dans le pipeline FAISS actuel.

