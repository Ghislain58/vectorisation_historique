import json
import re
from pathlib import Path

# Stopwords et termes trop gÃ©nÃ©riques qu'on ne veut PAS comme entitÃ©s
STOPWORDS_COMMON = {
    "nord",
    "sud",
    "dit",
    "dei",
    "domini",
    "conventus",
    "comte",
    "duc",
    "roi",
    "abbÃ©",
    "abbas",
    "episcopus",
    "dominus",
}


def clean_entries(entries, min_len=3, drop_if_numeric=True):
    cleaned = []

    for raw in entries:
        if raw is None:
            continue

        s = str(raw).strip()

        # enlÃ¨ve la pollution en dÃ©but/fin
        s = re.sub(r'^[\s;,\-]+', '', s)
        s = re.sub(r'[\s;,\-]+$', '', s)
        s = s.strip()
        if not s:
            continue

        # enlÃ¨ve parenthÃ¨ses fermantes
        s = s.replace(')', '').strip()

        # normalise "d'Auvergne" / "dâ€™Auvergne" -> "Auvergne"
        s = re.sub(r"^d['â€™]\s*", "", s, flags=re.IGNORECASE).strip()
        if not s:
            continue

        # supprime les entrÃ©es purement numÃ©riques
        if drop_if_numeric and re.fullmatch(r'\d+', s):
            continue

        # filtre les entrÃ©es trop courtes
        if len(s) < min_len:
            continue

        # stopwords gÃ©nÃ©riques (en minuscules ou capitalisÃ©s)
        if s.lower() in STOPWORDS_COMMON:
            continue

        cleaned.append(s)

    # dÃ©duplication
    uniq = []
    for s in cleaned:
        if s not in uniq:
            uniq.append(s)

    return uniq


def main():
    base = Path("mes_documents_historiques")
    src = base / "entity_lexicon_v2.json"
    dst = base / "entity_lexicon_v2_clean.json"

    if not src.exists():
        print(f"âŒ Fichier source introuvable : {src}")
        return

    print(f"ðŸ“‚ Chargement du lexique brut : {src}")
    data = json.load(src.open("r", encoding="utf-8"))

    raw_persons = data.get("persons", []) or []
    raw_places = data.get("places", []) or []
    raw_years = data.get("years", []) or []

    persons_clean = clean_entries(raw_persons, min_len=3, drop_if_numeric=True)
    places_clean = clean_entries(raw_places, min_len=3, drop_if_numeric=True)

    # les annÃ©es on les garde telles quelles, en les normalisant juste en strings
    years_clean = [str(y).strip() for y in raw_years if str(y).strip()]

    cleaned = {
        "persons": persons_clean,
        "places": places_clean,
        "years": years_clean,
    }

    print(f"âœ… Persons : {len(raw_persons)} -> {len(persons_clean)}")
    print(f"âœ… Places  : {len(raw_places)} -> {len(places_clean)}")
    print(f"âœ… Years   : {len(raw_years)} -> {len(years_clean)}")

    dst.write_text(json.dumps(cleaned, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"ðŸ’¾ Lexique nettoyÃ© Ã©crit dans : {dst}")


if __name__ == "__main__":
    main()
